server.port=${PORT:8080}
logging.level.org.atmosphere = warn
spring.mustache.check-template-location = false

# Launch the default browser when starting the application in development mode
vaadin.launch-browser=true

# Workaround for https://github.com/vaadin/hilla/issues/842
spring.devtools.restart.additional-exclude=dev/hilla/openapi.json
# To improve the performance during development.
# For more information https://vaadin.com/docs/flow/spring/tutorial-spring-configuration.html#special-configuration-parameters
vaadin.allowed-packages=com.vaadin,org.vaadin,dev.hilla,com.example.application
spring.jpa.defer-datasource-initialization = true

# LangChain4j default  properties
#langchain4j.open-ai.streaming-chat-model.api-key=${OPENAI_API_KEY}
#langchain4j.open-ai.streaming-chat-model.model-name=gpt-4o
#langchain4j.open-ai.streaming-chat-model.temperature=0
#langchain4j.open-ai.embedding-model.api-key=${OPENAI_API_KEY}
#langchain4j.open-ai.streaming-chat-model.strict-tools=true
#langchain4j.open-ai.streaming-chat-model.log-requests=true
#langchain4j.open-ai.streaming-chat-model.log-responses=false

# ollama
langchain4j.open-ai.chat-model.api-key=Ollama
langchain4j.open-ai.chat-model.model-name=qwen2.5:14b
langchain4j.open-ai.chat-model.log-requests=true
langchain4j.open-ai.chat-model.log-responses=true
langchain4j.open-ai.chat-model.base-url=http://localhost:11434/v1/
langchain4j.open-ai.chat-model.temperature=0.7
langchain4j.open-ai.streaming-chat-model.api-key=Ollama
langchain4j.open-ai.streaming-chat-model.base-url=http://localhost:11434/v1/
langchain4j.open-ai.streaming-chat-model.model-name=qwen2.5:14b
langchain4j.open-ai.streaming-chat-model.log-requests=true
langchain4j.open-ai.streaming-chat-model.log-responses=true


logging.level.dev.langchain4j=DEBUG
logging.level.dev.ai4j.openai4j=DEBUG
logging.level.ai.djl=OFF
